{
 "metadata": {
  "name": "",
  "signature": "sha256:c4d695bf398398ccb6f00252ce84e53a9c45a6dffcfbd9fe095ac0968d0cc905"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "#### Towards Combined Matrix and Tensor Factorization for Universal Schema Relation Extraction\n",
      "\n",
      "##### Abstract ##### \n",
      "> * Matrix factorization is useful for relation extraction in universal schema\n",
      "> * Dependencies between textual patterns and structured relations are encoded using low dimensional vectors for each entity pair.\n",
      "> * They are innacurate however for rare pairs or relations that depend crucially on the entity type \n",
      "> * Tensor factorization overcomes these for the task of link prediction by keeping entity wise factors however they are not suitable for universal schema task\n",
      "> However hybrid methods combining tensor and matrix factorization methods are shown to be fruitful.\n",
      "\n",
      "##### Introduction  \n",
      "* Riedel et al. convert knowledge base (KB) into binary matrix with entity pairs as rows and relations as columns.\n",
      "  * Factorizing this matrix results in low-dimensional factors for entity-pairs and relations.\n",
      "  * This combines multiple evidences for each entity pair and allows better prediction of unseen relations.\n",
      "  * However no information is shared among rows that contain the same entitiy.\n",
      "  * This impacts accuracy for pairs of entities not frequently seen together and relations that depend on fine-grained types (schoolAttended, nationality and book author.)\n",
      "* Tensor factorization for KB completion maintains per-entity factors that combine evidence from all relations an entity participates in to predict it's relation to other entities. \n",
      "  * These entity factors can be quite effective in identifying latent fine-grained entity types.\n",
      "  * Directly applying tensor factorization to universal scheme has not been successful. \n",
      "  * In this paper, authors explore application of matrix and tensor factorization for universal schema.\n",
      "\n",
      "##### Matrix and Tensor Factorization\n",
      "###### Universal Schema \n",
      "* Defined as union of all OpenIE-like surface form patterns found in text and fixed canonical relations that exist in a knowledge base. \n",
      "\n",
      " * The types are the union of all available types from all input sources, including multiple pre-existing ontologies and naturally-occurring textual surface-form expressions that indicate entity type, such as appositives, isa-expressions, or even adjectival or verb phrases. ([Universal Schema for Entity Type Prediction])(http://dl.acm.org/citation.cfm?id=2509572)\n",
      " eg:- James Cameron may appear as a person/director in Freebase, as a PERSON in TAC/KBP, and as a _movie-mogul_, _Canadian citizen_, and _jerk_ in various appositives in available text.\n",
      " \n",
      "* The task here is to complete this schema by jointly reasoning over surface form patterns and relations.\n",
      " * Each row in the matrix is an entity. Each column is an entity type. The task is to \"fill in\" the empty cells in the matrix.\n",
      "\n",
      "###### Matrix Factorization with Factors over Entity-Pairs\n",
      "* Reidel et al. construct a sparse binary matrix of size $|\\mathcal P| * |\\mathcal R|$ where rows indexed by $(a,b) \\in \\mathcal P$ and columns by surface form and freebase relations $s \\in \\mathcal R$\n",
      "\n",
      "* Generalized PCA is used to find a rank k factorization of with relation factors $\\mathbf r \\in \\mathbb R^{|\\mathcal R| \\times k}$ and entity-pair factors $\\mathbf p \\in \\mathbb R^{|\\mathcal P| \\times k} $ where $$P(s(a,b)) = \\sigma (\\mathbf r_{s} . \\mathbf p_{ab}) $$\n",
      "\n",
      "* Using this factorization, similar entity-pairs and relations are embedded close to each other in a k-dimensional vector space.\n",
      "\n",
      "* Since the observed data matrix contains only true entries, the parameters are learned using Bayesian personalized Ranking that supports implicit feedback.\n",
      "\n",
      "* Shortcomings - To learn a representation of an entity pair, needs to see large number of evidence pairs. This also boosts the number of parameters. \n",
      "\n",
      "###### Tensor Factorization with Entity Factors\n",
      "* Idea is that instead of a matrix where rows are entity pairs, they now use a mode 3 tensor. One for the relation, one for entities as first argument of the relation and last mode for entities as second argument of relation. \n",
      "\n",
      "* Notation: $\\mathbf e_{a} \\in \\mathbb R^{k}$ refers to the embedding of entity $a$.I n cases where the position of the entity requires different embeddings, we use $\\mathbf e_{a, 1}$ and $\\mathbf e_{a,2}$ to represent its occurrence as first and second argument,respectively. \n",
      "\n",
      "###### CANDECOMP/PARAFAC-Decomposition\n",
      "* CP decomposition (Canonical/Parallel Factors Decomposition) involves approxomating a tensor by a sum of component rank 1 tensors. This is the tensor equivalent of SVD.\n",
      " $$ \\mathcal X \\in \\mathbb R^{I \\times J \\times K} \\approx \\sum_{r=1}^{R}a_{r} \\circ b_{r} \\circ c_{r} $$ where R > 0 and $a_{r} \\in \\mathbb R^{I} b_{r} \\in \\mathbb R^{J} c_{r} \\in \\mathbb R^{K}$\n",
      "\n",
      "* The authors use a logistic version of CP decomposition as prior work has show that sigmoids are beneficial for decomposing binary data.\n",
      "\n",
      " $$ P(s(a,b)) = \\sigma(\\sum_{k}r_{s}^{k}e_{a}^{k}e_{b}^{k}) $$\n",
      "\n",
      "##### Tucker2 Decomposition\n",
      "* This is the tensor equivalent of PCA. Tucker decomposition essentially involves factorizing a tensor into a core tensor multiplied by a maxtrix along each mode. \n",
      " $$  \\mathcal X \\in \\mathbb R^{I \\times J \\times K} \\approx \\mathcal G \\times_{1} A \\times_{2} B \\times_{3} C $$ where $\\mathbf A \\in \\mathbb R^{I \\times P} \\mathbf B \\in \\mathbb R^{J \\times Q} and \\mathbf C \\in \\mathbb R^{K \\times R}$. \n",
      " \n",
      " * $\\mathbf A,\\mathbf B,\\mathbf C$ each represent the principle component of that tensor along that mode$.\n",
      "\n",
      "* Again the authors use a modified version of Tucker Decomposition where the tensor is transformed with a sigmoid first. \n",
      "\n",
      "* To reduce computation however they make another change in that one of the matrices $C$ is always assumed to be an identity matrix. \n",
      " * This is called Tucker2 decomposition. Out of the three modes, they set the relational mode to be fixed as a $k\\times k$ matrix and the entities as $k$ vectors.\n",
      " $$ P(s(a,b)) = \\sigma((\\mathbf R_{s} \\times \\mathbf e_{a,1}). \\mathbf e_{b,2}) $$\n",
      " \n",
      "* They also talk about RESCAL tensor factorization. RESCAL factors a (usually sparse) three-way tensor X such that each frontal slice $X_{k}$ is factored into  $X_{k} = A * R_{k} * A^{T} $. The frontal slices of X are quadratic, possibly asymmetric N x N matrices. Usually, these matrices correspond to the sparse adjacency matrices of the relational graph for a particular relation in a multi-relational data set.\n",
      "\n",
      "###### TransE\n",
      "* Idea behind this is that  a relation $s$ between two entities $a$ and $b$ holds, that relation\u2019s vector representation $r_{s}$ should translate the representation $e_{a}$ to the second argument $e_{b}$ \n",
      "$$ score(s(a,b)) = -||(\\mathbf e_{a} + \\mathbf r_{s}) - \\mathbf e_{b}||_{2} $$\n",
      "\n",
      " * In TransE, relationships are represented as translations in the embedding space : if $(a, r_{s}, b)$ holds, then the embedding of the tail entity $b$ should be close to the embedding of  the head entity $a$ plus some vector that depends on the relationship. \n",
      "\n",
      "###### ModelE\n",
      "* This is just the entity model described in Reidel et al. viewed as tensor factorization. And the $arity$ of each relation $r$ is assumed to be 2\n",
      "$$ P(s(a,b)) = \\sigma (\\mathbf r_{s,1}.\\mathbf e_{a} + \\mathbf r_{s,2}.\\mathbf e_{b}) $$\n",
      "\n",
      "##### Combined Matrix and Tensor Factorization\n",
      "* Matrix factorization works well for universal schema but performs poorly with regards to sparsity and capturing latent entity types crucial for relation extraction.\n",
      "\n",
      "* Tensors compactly represent unary entity embeddings but do not adequately capture pair specific information required to model relationships.\n",
      "\n",
      "* Chang et al. observed that matrix factorization predictions needed to be merged with tensors for better models. \n",
      "\n",
      "* Why do tensor models fare so badly? \n",
      "\n",
      " <img src = \"fig1.png\", width=\"400px\" height=\"auto\">\n",
      " \n",
      " * Black relations - Random\n",
      " * Red relations - Wherever black relations exist (similar to borIn relation for each X was born in Y surface pattern)\n",
      " * Green between all pairs of entities of different types\n",
      " * Blue - There is a green and a black relation (represents relations in real data. ambiguous surface patterns)\n",
      " \n",
      "* Intuition is that MF should identify red relations (strong correllation)\n",
      "* Green is based on latent types of entities so TF should identify it\n",
      "\n",
      " <img src = \"fig2a.png\", width=\"400px\" height=\"auto\">\n",
      "  \n",
      "\n",
      "###### Hybrid Factorization Models\n",
      "\n",
      "* Combined Model (FE) (from Reidel et al.)\n",
      " \n",
      " $$ P(s(a,b)) = \\sigma(\\mathbf r_{s}.\\mathbf e_{a,b} + \\mathbf r_{s,1}.\\mathbf e_{a} + \\mathbf r_{s,2}.\\mathbf e_{b}) $$\n",
      " \n",
      "* Rectifier Model (RFE)\n",
      " * In the previous model one model could easily override the other. So they use a rectifier ($ \\oplus x = \\log(1 + e^{x}) $) to ensure that each needs to reach a threshold before influencing the total.\n",
      " $$ P(s(a,b)) = \\oplus(\\mathbf r_{s}. \\mathbf p_{ab}) \\oplus (\\mathbf r_{s,1}.\\mathbf e_{a} + \\mathbf r_{s,2}.\\mathbf e_{b})) $$\n",
      " \n",
      "##### Experiments \n",
      "* Synthetic RGB relations \n",
      " * Average precision as the rank is varied. FE and RFE perform as well (or better than) tensor factorization on Green and matrix factorization on Red, but importantly, are able to encode the Blue relations that matrix or tensor factorization fail to model\n",
      " * Remaining tensor approaches work similar to RESCAL and Model E\n",
      "\n",
      "* Universal Schema Extraction\n",
      " * Tensor factorization performs poorly. FE does better than the rest.\n",
      " * No idea why RFE performs worse than matrix factorization.\n",
      " * The Neighbourhood model for Reidel et al actually performs the best.\n",
      " \n",
      "##### Conclusions\n",
      "\n",
      "* Tensor based models perform poorly for Universal Schema for Relation Extraction.\n",
      "* Matrix factorization is appropriate for the task however it does not work well with sparse observations or identifying latent entity types.\n",
      "* Hybrid models work well with synthetic data (just one task) but do not show equivalent gains for real world data.\n",
      "* Hybrid models enable easy extensions to n-ary relations."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {
      "slideshow": {
       "slide_type": "slide"
      }
     },
     "source": [
      "####Addendum\n",
      "\n",
      "###### Matrix Factorization -> PCA\n",
      "* Given a relation $r \\in \\mathcal R$ and a tuple $t \\in \\mathcal T$ the pair $(r, t)$ is a fact or relation instance.  \n",
      "The input to the model is a set of observed facts $\\mathcal O_{t} := \\{ (r,t) \\in \\mathcal O \\}$ The objective is a model that for given relation $r$ and a tuple $t$ the probability $p(y_{r,t}) = 1$.   \n",
      "This can be modeled using a log linear model as $$ p(y_{r,t}=1 | \\theta_{t,t}) := \\sigma(\\theta_{r,t}) = \\frac{1}{1 + \\exp(-\\theta_{r,t})} $$\n",
      "\n",
      "* This can also be viewed as being similar to a problem of finding the probability that customer $t$ likes product $r$ and hence is similar in nature to that of collaborative filtering problem.\n",
      "\n",
      "###### Entity Model\n",
      "* Instead of using predetermined set of entity types, goal is to learn a latent entity representation from the data.  \n",
      "More concretely for each entity $e$ we have a feature vector $t_{e}$ of dimension $K^{E}$.  \n",
      "For each relation $r$ and argument slot $i$ a feature vector $d_{i}$ of the same dimension is introduced.   \n",
      "Measuring compatibility of an entity tuple and relation amounts to measuring, and summing up, compatibility between each argument slot representation and the corresponding entity representation.\n",
      "$$ \\theta_{r,t}^{E} := \\sum_{i=1}^{arity(r)} \\sum_{k}^{K^{E}}d_{i,k}t_{i,k} $$\n",
      "\n",
      "###### Parameter Estimation\n",
      "* Maximum Likelihood Estimation is not a valid option as there is no negative data.  \n",
      " * Another approach was to sample set of unobserved facts as negative data but it was sensitive to choice of negative data and increased runtime significantly.\n",
      "\n",
      "* This is where Bayesian Personalized Ranking (BPR) comes in. In this rather than 0/1 classification goal is to give observed true facts higher scores than unobserved (true or false) facts.\n",
      "\n",
      "* Objective is to rank observed facts above unobserved ones.  \n",
      " * For each relation $r$ and each observed fact $f^{+} := (r,t^{+}) \\in \\mathcal O$ we choose all tuples $t^{-}$ such that $f^{-} := (r, t^{-}) \\notin \\mathcal O$. \n",
      " * For each pair of facts $f^{+}$ and $f^{-}$ we want $p(f^{+}) > p(f^{-})$ and hence $\\theta_{f^{+}} > \\theta_{f^{-}}$ by maximizing a sum of terms of the form $\\text{Obj}_{f^{+},f^{-}} := \\log (\\sigma(\\theta_{f^{+}} - \\theta_{f^{-}} ))$ $$ \\text{Obj} := \\sum_{(r,t^{+}) \\in \\mathcal O} \\sum_{(r, t^{-}) \\notin \\mathcal O} \\text{Obj}_{(r, t^{+})} \\text{Obj}_{(r, t^{-})} $$\n",
      " * This is optimized using SGD where in each epoch |\\mathcal O| facts are sampled with replacement from $\\mathcal O$. For\n",
      "each sampled fact $(r, t^{+})$ we then sample a tuple $t^{-} \\in \\mathcal T$ such that $(r, t^{-}) \\notin \\mathcal O$ is not an observed fact. This gives us $|\\mathcal O|$ fact pairs  $f^{+}$ and $f^{-}$ and for each pair we do an SGD update using the corresponding gradients of Obj$_{f^{+} ,f^{\u2212}}$.\n",
      "\n",
      "###### TransE \n",
      "* Given a training set $S$ of triplets $(h, l, t)$ composed of 2 entities $h$ and $t$ and a relationship $l$ the energy of a triplet is defined as $d(h+l, t)$ for some dissimilarity measure $d$ (either L1/L2 norm). Objective is to minimize a margin based ranking criterion over the training set \n",
      "$$\\mathcal L = \\sum_{(h,l, t) \\in S} \\sum_{(h^{'}, l, t^{'}) \\in S^{'}_{h,l,t}}[\\gamma + d(h+l, t) - d(h^{'} + l, t^{'})]_{+} $$\n",
      "where $[x]_{+}$ denotes the positive part of $x$, $\\gamma > 0$ denotes the hyperparameter and \n",
      "$$ S^{'}_{(h, l, t)} = \\{(h^{'}, l, t)| h^{'} \\in E\\} \\cup \\{(h, l, t^{'}) | t^{'} \\in E\\} $$\n",
      "\n",
      "* This is trained with SGD in minibatch mode\n"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}